---
title: "Progress"
date: 2018-09-03T11:41:34+02:00
---

What I have done:
- Lancaster Kickoff
- phd funding application
- Start ideas on making a meta-recommender based on pretrained vectors
- Reading&ex in course
- Partly on side: Implemented a Recurrent NN recommender that tries to predict probability of what item will be consumed next. In A/B testing now. (http://dx.doi.org/10.1145/3018661.3018689)

Next steps and ideas as discussed (and self planned) after Lancaster kick-off:
- finish funding application
- Bayesianify the meta model in some way.
- One problem is that these submodels does not contain any really «large» uncertainty: A bad submodel may give a score of 0.5 to one relevant and one irrelevant item. The meta model cant really use this to determine between the two.
- The real uncertainty may be within the submodel
- Might be more interesting to consider the vector spaces from submodels and their translation into a reward function (click prob). More general, and more usable in different scenarios. Also allows the action space to be all items in inventory, not just (top k)*(# submodels).
- 	For this missing vectors need to be considered.
- 	Is it a problem that training data may be older than vector generation?
- 	Should this «super model» contain latent variables itself?
-      The "agility" of models such as "similar to last seen" and "Popular" will not be able to be as dynamic in the new setup.. this will hurt relevance during production.

Other interesting things Id like to try:
- How to use mcmc when data increase/change?
