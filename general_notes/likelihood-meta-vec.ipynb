{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Meta vector model (with no normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1\n",
    "We have a dataset of covariates $ (x_u, y_i)$ and a binary response variable $C = [0,1]$.\n",
    "\n",
    "Assume that a user u has a probability $\\theta_{ui}$ to click on a particular item i.\n",
    "\n",
    "$$ \\theta_{ui} = Pr(click | data, par) = Pr(C = c | (x_u, y_i), W) = \\sigma( x_i W y_i) $$\n",
    "\n",
    "where $\\sigma(x) = \\frac{exp(x)}{1+exp(x)}$ is the logit transform, and $W \\in \\Re^{d* d}$ is the model's parameters.\n",
    "\n",
    "## Likelihood function\n",
    "\n",
    "The likelihood of one observations is $ P(c_k | W) =  \\theta_k^{c_k} (1-\\theta_k) ^{1-c_k} $.\n",
    "Assuming that each observation is independent, the log likelihood can be written as\n",
    "\n",
    "$$ loglik = \\sum_{i=1}^{n}  c_k ln(\\theta_k) + (1-w_k)ln(1-\\theta_k) $$\n",
    "\n",
    "where $\\theta_k = \\theta_{u,i} = \\sigma( x_i W y_i)$.\n",
    "\n",
    "## Prior\n",
    "We know from before that the product $x_u^T y_i$ is correlated with relevance. \n",
    "Therefore, it is natural to put a prior of 1 on the diagonal and 0 elsewhere.\n",
    "For simplicity, assume all entries in $W$ are independent.\n",
    "\n",
    "$$P(w_{ij}) = N( 1_{i=j}, \\sigma_0)$$\n",
    "\n",
    "where $\\sigma_0$ is some constant. e.g. $\\sigma_0 = 1$\n",
    "\n",
    "## Posterior\n",
    "\n",
    "$$ lnP(W | data) = loglik + \\sum_{i = 0}^d \\sum_{j = i}^d lnP(w_{ij})$$\n",
    "\n",
    "\n",
    "## Restrictions:\n",
    "- $|Wy_i| = 1$ or $|x_i W| = 1$ in order for normalized dot product to work in production.\n",
    "- This model does not consider normalization factors. Expand model to take this into account.\n",
    "- A dimension of 300 in the final model is too big to be able to run in production.\n",
    "Need to downsample it to managable size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 (Work In Progress)\n",
    "\n",
    "We need to fix the limitations of the first model. \n",
    "- We need to have vectors that has norm equal to one.\n",
    "- We need to add normalizing variables for each normalizing variable. We will encode all these as one-hot in the model.\n",
    "- Further we need to combine the score from the user/item context and the normalizing constant. We decide to do that by adding a linear layer that takes the concatinated sub-results as input.\n",
    "\n",
    "We have a dataset of following covariates:  \n",
    "$x_u \\in \\Re^{d_{in}}$ is a user vector of dimension $d_{in}$,  \n",
    "$y_i \\in \\Re^{d_{in}}$ is a item vector of dimension $d_{in}$,  \n",
    "$h \\in \\Re^{24}$ a one-hot encoded categorical variable of what hour the event happened,  \n",
    "$p \\in \\Re^{48}$ a one-hot categorical variable of what position the event happened,  \n",
    "$w \\in \\Re^{7}$ a one-hot categorical variable of what weekday the event happened,  \n",
    "$d \\in \\Re^{3}$ a one-hot categorical variable of what device the event happened,  \n",
    "$C = [0,1]$ is a binary response variable.\n",
    "\n",
    "$$ h_u(x) = \\frac{W_u x}{\\sqrt{|W_u x|}} \\in \\Re^{d_{out}}$$\n",
    "\n",
    "$$h_i(y) = \\frac{W_i y}{\\sqrt{|W_i y|}} \\in \\Re^{d_{out}}$$\n",
    "\n",
    "for parameters $W_i, W_u \\in \\Re^{d_{out},d_{in}}$ \n",
    "\n",
    "For all the normalizing factors:\n",
    "$$h_p = W_p p \\in \\Re^{1}$$\n",
    "$$h_h = W_h h \\in \\Re^{1}$$\n",
    "$$h_w = W_w w \\in \\Re^{1}$$\n",
    "$$h_d = W_d d \\in \\Re^{1}$$\n",
    "\n",
    "for parameters $W_h \\in \\Re^{24}$, $W_p \\in \\Re^{48}$, $W_w \\in \\Re^{7}$, $W_d \\in \\Re^{3}$.\n",
    "\n",
    "\n",
    "Concatingating all the values we have gathered above we get:\n",
    "$$ H_1 = [h_u(x)^t h_i(y), h_p, h_h, h_w, h_d]^T$$\n",
    "\n",
    "Doing a linear transform to create logits for click/no click:\n",
    "$$ H_2 = W_1 H_1 \\in \\Re^{2}$$\n",
    "for parameter $W_1 \\in \\Re^{2 * 5}$.\n",
    "\n",
    "Then we can define the probability of click as \n",
    "\n",
    "$$ \\theta_{ui} = Pr(click | data, par) = Pr(C = c | (x_u, y_i), W) = \\sigma( H_2) $$\n",
    "\n",
    "where $\\sigma(x) = \\frac{e^{x_1}}{e^{x_1} + e^{x_2}}$ for a vector x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Likelihood function\n",
    "Let W be the collection of all weights defined above. The likelihood of one observations is $ P(c_k | W) =  \\theta_k^{c_k} (1-\\theta_k) ^{1-c_k} $.\n",
    "Assuming that each observation is independent, the log likelihood can be written as\n",
    "\n",
    "$$ loglik = \\sum_{i=1}^{n}  c_k ln(\\theta_k) + (1-w_k)ln(1-\\theta_k) $$\n",
    "\n",
    "where $\\theta_k = \\theta_{u,i} = \\sigma( H_2)$. That is such a messy equation that I do not think it will help to write it out...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc-showcode": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
